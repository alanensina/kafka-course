Before to start:
- Download the last recommended version of Kafka on: https://kafka.apache.org/downloads
- Extract the .tgz
- The scripts used will be found on /bin and the configuration files are on /config

Topics:
- Topic is an Entity in Kafka with name (just like a table on databases)
- Topics are into a Kafka Broker
- Kafka Producer send a message to a topic and the Kafka Consumer will consume a message that are retained in the topic

Partitions:
- Partition is where the message lives insed the topic
- Each topic will be created with one or more partitions
- Each partition is independent of each other
- The Kafka Producer is in charge to decides which partition the message will be stored

- Usefull commands in Kafka CLI:
https://github.com/dilipsundarraj1/kafka-for-developers-using-spring-boot/blob/master/SetUpKafka.md

Setting up the ZooKeeper:
- Get in /bin on Terminal
- Start up the ZooKeeper: ./zookeeper-server-start.sh ../config/zookeeper.properties
- Add the below properties in the server.properties:
    listeners=PLAINTEXT://localhost:9092
    auto.create.topics.enable=false
- Get in /bin on a new Terminal
- Start up the Kafka Broker: ./kafka-server-start.sh ../config/server.properties

Creating a topic, producing and consuming a message:
- Make sure that the ZooKeeper and Kafka is running
- Get in /bin on a new Terminal
- Create a topic: ./kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 4 --topic test-topic
- Instantiate a Kafka Producer (without key): ./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic
- Type any message that you want into the terminal
- Get in /bin on a new Terminal
- Instantiate a Kafka Consumer (without key): ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning
- All the messages that you sent on Producer will be consumed and shown on this terminal

Messages with keys:
- When a producer sends a message with a key, the Partitioner will orient the message to a Partition that defined a key, it's usefull if you want to define which Partition to store and organize your messages
- Get in /bin on a new Terminal
- Create a topic: ./kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 4 --topic test-topic
- Set the key separator: ./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic --property "key.separator=-" --property "parse.key=true"
- Note the params key.separator that defines a separator used to split the message and the key, so the messages must be sent like this:
    A-Alan
    A-Apple
    A-Aerosmith
- Get in /bin on a new Terminal
- Instantiate a Kafka Consumer with the key separator that matches with Producer: ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning -property "key.separator= - " --property "print.key=true"
- All the messages will be shown with the key, if you don't want to see the key, set the print.key parameter to false

Consumer groups:
- Consumer groups are used for scalable message consumption
- Each different application will have a unique consumer group
- Who manages the consumer group?
    Kafka Broker manages the consumer-groups and acts as a Group Co-ordinator
- To list consumer-groups, get in /bin and send the command: ./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
- Everytime that you Instantiate a new consumer, a new id will be generated
- To Instantiate a consumer with a group, get in /bin and send a command: ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --group <group-name>

Commit log and Retention Policy:
- You can find all the logs into the folder of the kafka, but there's a hidden folder called /tmp that is designed to store logs from the producers and consumers and you can set the retention on the server.properties. The default retention is 168 hours (7 days)
- To read the logs into the terminal send the command: ./kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --files <path+log-name>

Kafka as a Distributed System:
- Client requests are distributed between brokers
- Easy to scale by adding more brokers based on the need
- Handles data loss using replication

Kafka Brokers:
- To set up multiple brokers you need to configure new server.properties adding the following code:
    broker.id=<unique-broker-d>
    listeners=PLAINTEXT://localhost:<unique-port>
    log.dirs=/tmp/<unique-kafka-folder>
    auto.create.topics.enable=false
- To start the new broker use the following commando: ./kafka-server-start.sh ../config/<server properties name that you created, e.g: server-1.properties>
- Create the topics following the amount of brokers: ./kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 3 --topic test-topic-replicated
- Instantiate the producer: ./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic-replicated
- Instantiate the consumer: ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic-replicated --from-beginning
- Send messages and then check the logs in the /tmp